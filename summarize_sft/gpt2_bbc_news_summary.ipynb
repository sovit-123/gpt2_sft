{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U transformers\n",
    "# !pip install -U datasets\n",
    "# !pip install tensorboard\n",
    "# !pip install sentencepiece\n",
    "# !pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    AutoModelForCausalLM\n",
    ")\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['File_path', 'Articles', 'Summaries'],\n",
      "    num_rows: 1779\n",
      "})\n",
      "Dataset({\n",
      "    features: ['File_path', 'Articles', 'Summaries'],\n",
      "    num_rows: 445\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('gopalkalpande/bbc-news-summary', split='train')\n",
    "full_dataset = dataset.train_test_split(test_size=0.2, shuffle=True)\n",
    "dataset_train = full_dataset['train']\n",
    "dataset_valid = full_dataset['test']\n",
    " \n",
    "print(dataset_train)\n",
    "print(dataset_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'File_path': 'politics', 'Articles': 'Borders rail link campaign rally..Campaigners are to stage a rally calling for a Borders rail link which was closed in 1969 to be reopened...They will mark the 36th anniversary of the line closure, which ran from Edinburgh through the Borders and on to Carlisle, with a walk at Tweedbank. Anne Borthwick, of Campaign for Borders Rail, said reopening the Waverley Line would restore the area\\'s prosperity. MSPs are considering the reintroduction of passenger rail services through Midlothian to the Borders. Campaigners have said that reopening the Waverley Line, which could cost up to £100m, would be a huge economic boost for the Borders...In 2000, Borders Council said the area\\'s economy had suffered since the closure. Ms Borthwick said the lobby group was determined to keep the pressure on the Scottish Executive. \"We are hoping that many people will join us in a march to mark the 36th anniversary of the closure of the Waverley Line,\" she said. \"Campaign for Borders Rail is the biggest independent lobby group in Scotland and we have been lobbying tirelessly for the reinstatement of rail services to the Borders and eventually to Carlisle...\"We believe that it is time for the Scottish Executive to commit to the first phase of the project by pledging to fund the line between Edinburgh and Tweedbank in the first instance and then investigate extending the line in the future.\" Ms Borthwick said reopening the line would be a prosperous move and protect the character of the Scottish Borders. A study in 2000, which was commissioned by the executive, Scottish Borders Council, Midlothian Council and Scottish Borders Enterprise, found that a half-hourly service from Tweedbank to Edinburgh could cover its operating costs. It also found that a half-hourly service from Gorebridge to Edinburgh could cover operating costs and that a freight railway joining the West Coast Main Line at Longtown could also be reinstated.', 'Summaries': 'Anne Borthwick, of Campaign for Borders Rail, said reopening the Waverley Line would restore the area\\'s prosperity.Ms Borthwick said reopening the line would be a prosperous move and protect the character of the Scottish Borders.Campaigners have said that reopening the Waverley Line, which could cost up to £100m, would be a huge economic boost for the Borders.A study in 2000, which was commissioned by the executive, Scottish Borders Council, Midlothian Council and Scottish Borders Enterprise, found that a half-hourly service from Tweedbank to Edinburgh could cover its operating costs.They will mark the 36th anniversary of the line closure, which ran from Edinburgh through the Borders and on to Carlisle, with a walk at Tweedbank.\"Campaign for Borders Rail is the biggest independent lobby group in Scotland and we have been lobbying tirelessly for the reinstatement of rail services to the Borders and eventually to Carlisle.'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'gpt2'\n",
    "BATCH_SIZE = 4\n",
    "NUM_PROCS = os.cpu_count()\n",
    "EPOCHS = 20\n",
    "OUT_DIR = 'results_gpt2_bbc_news_summary'\n",
    "MAX_LENGTH = 512 # Maximum context length to consider while preparing dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = dataset_train[10]['Articles'] + ' TL;DR: ' + dataset_train[10]['Summaries']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"IBM puts cash behind Linux push..IBM is spending $100m (£52m) over the next three years beefing up its commitment to Linux software...The cash injection will be used to help its customers use Linux on every type of device from handheld computers and phones right up to powerful servers. IBM said the money will fund a variety of technical, research and marketing initiatives to boost Linux use. IBM said it had taken the step in response to greater customer demand for the open source software...In 2004 IBM said it had seen double digit growth in the number of customers using Linux to help staff work together more closely. The money will be used to help this push towards greater collaboration and will add Linux-based elements to IBM's Workplace software. Workplace is a suite of programs and tools that allow workers to get at core business applications no matter what device they use to connect to corporate networks. One of the main focuses of the initiative will be to make it easier to use Linux-based desktop computers and mobile devices with Workplace. Even before IBM announced this latest spending boost it was one of the biggest advocates of the open source way of working. In 2001 it put $300m into a three-year Linux program and has produced Linux versions of many of its programs. Linux and the open source software movement are based on the premise that developers should be free to tinker with the core components of software programs. They reason that more open scrutiny of software produces better programs and fuels innovation. TL;DR: IBM said it had taken the step in response to greater customer demand for the open source software.Linux and the open source software movement are based on the premise that developers should be free to tinker with the core components of software programs.IBM is spending $100m (£52m) over the next three years beefing up its commitment to Linux software.IBM said the money will fund a variety of technical, research and marketing initiatives to boost Linux use.The cash injection will be used to help its customers use Linux on every type of device from handheld computers and phones right up to powerful servers.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(example):\n",
    "    context = f\"{example['Articles'] + ' TL;DR: ' + example['Summaries']}\"\n",
    "    final_tokens = tokenizer(context, max_length=MAX_LENGTH, padding='max_length')\n",
    "    return final_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "544730a630a14f71b1867138fb0c13a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/1779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f7f5728e785476eb0d87438f14bc7e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/445 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train = dataset_train.map(\n",
    "    preprocess_function,\n",
    "#     batched=True,\n",
    "    num_proc=NUM_PROCS,\n",
    "    remove_columns=dataset_train.column_names,\n",
    ")\n",
    "tokenized_valid = dataset_valid.map(\n",
    "    preprocess_function,\n",
    "#     batched=True,\n",
    "    num_proc=NUM_PROCS,\n",
    "    remove_columns=dataset_valid.column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [33, 6361, 6787, 2792, 1923, 7903, 492, 46102, 364, 389, 284, 3800, 257, 7903, 4585, 329, 257, 40934, 6787, 2792, 543, 373, 4838, 287, 16450, 284, 307, 37415, 986, 2990, 481, 1317, 262, 4570, 400, 11162, 286, 262, 1627, 16512, 11, 543, 4966, 422, 23475, 832, 262, 40934, 290, 319, 284, 8124, 20919, 11, 351, 257, 2513, 379, 24205, 276, 17796, 13, 15397, 347, 1506, 16239, 11, 286, 13718, 329, 40934, 12950, 11, 531, 302, 29443, 262, 370, 8770, 1636, 6910, 561, 11169, 262, 1989, 338, 19519, 13, 337, 4303, 82, 389, 6402, 262, 38368, 596, 286, 11849, 6787, 2594, 832, 7215, 75, 849, 666, 284, 262, 40934, 13, 13718, 364, 423, 531, 326, 302, 29443, 262, 370, 8770, 1636, 6910, 11, 543, 714, 1575, 510, 284, 4248, 3064, 76, 11, 561, 307, 257, 3236, 3034, 5750, 329, 262, 40934, 986, 818, 4751, 11, 40934, 4281, 531, 262, 1989, 338, 3773, 550, 6989, 1201, 262, 16512, 13, 6997, 347, 1506, 16239, 531, 262, 10866, 1448, 373, 5295, 284, 1394, 262, 3833, 319, 262, 11905, 10390, 13, 366, 1135, 389, 7725, 326, 867, 661, 481, 4654, 514, 287, 257, 9960, 284, 1317, 262, 4570, 400, 11162, 286, 262, 16512, 286, 262, 370, 8770, 1636, 6910, 553, 673, 531, 13, 366, 46102, 329, 40934, 12950, 318, 262, 4094, 4795, 10866, 1448, 287, 8838, 290, 356, 423, 587, 17502, 47905, 329, 262, 6865, 26090, 286, 6787, 2594, 284, 262, 40934, 290, 4191, 284, 8124, 20919, 9313, 1135, 1975, 326, 340, 318, 640, 329, 262, 11905, 10390, 284, 4589, 284, 262, 717, 7108, 286, 262, 1628, 416, 43858, 284, 1814, 262, 1627, 1022, 23475, 290, 24205, 276, 17796, 287, 262, 717, 4554, 290, 788, 9161, 16610, 262, 1627, 287, 262, 2003, 526, 6997, 347, 1506, 16239, 531, 302, 29443, 262, 1627, 561, 307, 257, 32757, 1445, 290, 1805, 262, 2095, 286, 262, 11905, 40934, 13, 317, 2050, 287, 4751, 11, 543, 373, 20462, 416, 262, 4640, 11, 11905, 40934, 4281, 11, 7215, 75, 849, 666, 4281, 290, 11905, 40934, 14973, 11, 1043, 326, 257, 2063, 12, 9769, 306, 2139, 422, 24205, 276, 17796, 284, 23475, 714, 3002, 663, 5361, 3484, 13, 632, 635, 1043, 326, 257, 2063, 12, 9769, 306, 2139, 422, 17557, 9458, 284, 23475, 714, 3002, 5361, 3484, 290, 326, 257, 30724, 20515, 9679, 262, 2688, 8545, 8774, 6910, 379, 5882, 12735, 714, 635, 307, 40685, 13, 24811, 26, 7707, 25, 15397, 347, 1506, 16239, 11, 286, 13718, 329, 40934, 12950, 11, 531, 302, 29443, 262, 370, 8770, 1636, 6910, 561, 11169, 262, 1989, 338, 19519, 13, 10128, 347, 1506, 16239, 531, 302, 29443, 262, 1627, 561, 307, 257, 32757, 1445, 290, 1805, 262, 2095, 286, 262, 11905, 40934, 13, 46102, 364, 423, 531, 326, 302, 29443, 262, 370, 8770, 1636, 6910, 11, 543, 714, 1575, 510, 284, 4248, 3064, 76, 11, 561, 307, 257, 3236, 3034, 5750, 329, 262, 40934, 13, 32, 2050, 287, 4751, 11, 543, 373, 20462, 416, 262, 4640, 11, 11905, 40934, 4281, 11, 7215, 75, 849, 666, 4281, 290, 11905, 40934, 14973, 11, 1043, 326, 257, 2063, 12, 9769, 306, 2139, 422, 24205, 276, 17796, 284, 23475, 714, 3002, 663, 5361, 3484, 13, 2990, 481, 1317, 262, 4570, 400, 11162, 286, 262, 1627, 16512, 11, 543, 4966, 422, 23475, 832, 262, 40934, 290, 319, 284, 8124, 20919, 11, 351, 257, 2513, 379, 24205, 276, 17796, 526, 46102, 329, 40934, 12950, 318, 262, 4094, 4795, 10866, 1448, 287, 8838, 290, 356, 423, 587, 17502, 47905, 329, 262, 6865, 26090, 286, 6787, 2594, 284, 262, 40934, 290, 4191, 284, 8124, 20919, 13], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    if total_length >= MAX_LENGTH:\n",
    "        total_length = (total_length // MAX_LENGTH) * MAX_LENGTH\n",
    "    result = {\n",
    "        k: [t[i : i + MAX_LENGTH] for i in range(0, total_length, MAX_LENGTH)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "274a945061af46f4821621e872676975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/1779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "532093c8532a492a8efc58d3e80ee4bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/445 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm_dataset_train = tokenized_train.map(\n",
    "    group_texts, num_proc=NUM_PROCS, batched=True\n",
    ")\n",
    "lm_dataset_valid = tokenized_valid.map(\n",
    "    group_texts, num_proc=NUM_PROCS, batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [33, 6361, 6787, 2792, 1923, 7903, 492, 46102, 364, 389, 284, 3800, 257, 7903, 4585, 329, 257, 40934, 6787, 2792, 543, 373, 4838, 287, 16450, 284, 307, 37415, 986, 2990, 481, 1317, 262, 4570, 400, 11162, 286, 262, 1627, 16512, 11, 543, 4966, 422, 23475, 832, 262, 40934, 290, 319, 284, 8124, 20919, 11, 351, 257, 2513, 379, 24205, 276, 17796, 13, 15397, 347, 1506, 16239, 11, 286, 13718, 329, 40934, 12950, 11, 531, 302, 29443, 262, 370, 8770, 1636, 6910, 561, 11169, 262, 1989, 338, 19519, 13, 337, 4303, 82, 389, 6402, 262, 38368, 596, 286, 11849, 6787, 2594, 832, 7215, 75, 849, 666, 284, 262, 40934, 13, 13718, 364, 423, 531, 326, 302, 29443, 262, 370, 8770, 1636, 6910, 11, 543, 714, 1575, 510, 284, 4248, 3064, 76, 11, 561, 307, 257, 3236, 3034, 5750, 329, 262, 40934, 986, 818, 4751, 11, 40934, 4281, 531, 262, 1989, 338, 3773, 550, 6989, 1201, 262, 16512, 13, 6997, 347, 1506, 16239, 531, 262, 10866, 1448, 373, 5295, 284, 1394, 262, 3833, 319, 262, 11905, 10390, 13, 366, 1135, 389, 7725, 326, 867, 661, 481, 4654, 514, 287, 257, 9960, 284, 1317, 262, 4570, 400, 11162, 286, 262, 16512, 286, 262, 370, 8770, 1636, 6910, 553, 673, 531, 13, 366, 46102, 329, 40934, 12950, 318, 262, 4094, 4795, 10866, 1448, 287, 8838, 290, 356, 423, 587, 17502, 47905, 329, 262, 6865, 26090, 286, 6787, 2594, 284, 262, 40934, 290, 4191, 284, 8124, 20919, 9313, 1135, 1975, 326, 340, 318, 640, 329, 262, 11905, 10390, 284, 4589, 284, 262, 717, 7108, 286, 262, 1628, 416, 43858, 284, 1814, 262, 1627, 1022, 23475, 290, 24205, 276, 17796, 287, 262, 717, 4554, 290, 788, 9161, 16610, 262, 1627, 287, 262, 2003, 526, 6997, 347, 1506, 16239, 531, 302, 29443, 262, 1627, 561, 307, 257, 32757, 1445, 290, 1805, 262, 2095, 286, 262, 11905, 40934, 13, 317, 2050, 287, 4751, 11, 543, 373, 20462, 416, 262, 4640, 11, 11905, 40934, 4281, 11, 7215, 75, 849, 666, 4281, 290, 11905, 40934, 14973, 11, 1043, 326, 257, 2063, 12, 9769, 306, 2139, 422, 24205, 276, 17796, 284, 23475, 714, 3002, 663, 5361, 3484, 13, 632, 635, 1043, 326, 257, 2063, 12, 9769, 306, 2139, 422, 17557, 9458, 284, 23475, 714, 3002, 5361, 3484, 290, 326, 257, 30724, 20515, 9679, 262, 2688, 8545, 8774, 6910, 379, 5882, 12735, 714, 635, 307, 40685, 13, 24811, 26, 7707, 25, 15397, 347, 1506, 16239, 11, 286, 13718, 329, 40934, 12950, 11, 531, 302, 29443, 262, 370, 8770, 1636, 6910, 561, 11169, 262, 1989, 338, 19519, 13, 10128, 347, 1506, 16239, 531, 302, 29443, 262, 1627, 561, 307, 257, 32757, 1445, 290, 1805, 262, 2095, 286, 262, 11905, 40934, 13, 46102, 364, 423, 531, 326, 302, 29443, 262, 370, 8770, 1636, 6910, 11, 543, 714, 1575, 510, 284, 4248, 3064, 76, 11, 561, 307, 257, 3236, 3034, 5750, 329, 262, 40934, 13, 32, 2050, 287, 4751, 11, 543, 373, 20462, 416, 262, 4640, 11, 11905, 40934, 4281, 11, 7215, 75, 849, 666, 4281, 290, 11905, 40934, 14973, 11, 1043, 326, 257, 2063, 12, 9769], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [33, 6361, 6787, 2792, 1923, 7903, 492, 46102, 364, 389, 284, 3800, 257, 7903, 4585, 329, 257, 40934, 6787, 2792, 543, 373, 4838, 287, 16450, 284, 307, 37415, 986, 2990, 481, 1317, 262, 4570, 400, 11162, 286, 262, 1627, 16512, 11, 543, 4966, 422, 23475, 832, 262, 40934, 290, 319, 284, 8124, 20919, 11, 351, 257, 2513, 379, 24205, 276, 17796, 13, 15397, 347, 1506, 16239, 11, 286, 13718, 329, 40934, 12950, 11, 531, 302, 29443, 262, 370, 8770, 1636, 6910, 561, 11169, 262, 1989, 338, 19519, 13, 337, 4303, 82, 389, 6402, 262, 38368, 596, 286, 11849, 6787, 2594, 832, 7215, 75, 849, 666, 284, 262, 40934, 13, 13718, 364, 423, 531, 326, 302, 29443, 262, 370, 8770, 1636, 6910, 11, 543, 714, 1575, 510, 284, 4248, 3064, 76, 11, 561, 307, 257, 3236, 3034, 5750, 329, 262, 40934, 986, 818, 4751, 11, 40934, 4281, 531, 262, 1989, 338, 3773, 550, 6989, 1201, 262, 16512, 13, 6997, 347, 1506, 16239, 531, 262, 10866, 1448, 373, 5295, 284, 1394, 262, 3833, 319, 262, 11905, 10390, 13, 366, 1135, 389, 7725, 326, 867, 661, 481, 4654, 514, 287, 257, 9960, 284, 1317, 262, 4570, 400, 11162, 286, 262, 16512, 286, 262, 370, 8770, 1636, 6910, 553, 673, 531, 13, 366, 46102, 329, 40934, 12950, 318, 262, 4094, 4795, 10866, 1448, 287, 8838, 290, 356, 423, 587, 17502, 47905, 329, 262, 6865, 26090, 286, 6787, 2594, 284, 262, 40934, 290, 4191, 284, 8124, 20919, 9313, 1135, 1975, 326, 340, 318, 640, 329, 262, 11905, 10390, 284, 4589, 284, 262, 717, 7108, 286, 262, 1628, 416, 43858, 284, 1814, 262, 1627, 1022, 23475, 290, 24205, 276, 17796, 287, 262, 717, 4554, 290, 788, 9161, 16610, 262, 1627, 287, 262, 2003, 526, 6997, 347, 1506, 16239, 531, 302, 29443, 262, 1627, 561, 307, 257, 32757, 1445, 290, 1805, 262, 2095, 286, 262, 11905, 40934, 13, 317, 2050, 287, 4751, 11, 543, 373, 20462, 416, 262, 4640, 11, 11905, 40934, 4281, 11, 7215, 75, 849, 666, 4281, 290, 11905, 40934, 14973, 11, 1043, 326, 257, 2063, 12, 9769, 306, 2139, 422, 24205, 276, 17796, 284, 23475, 714, 3002, 663, 5361, 3484, 13, 632, 635, 1043, 326, 257, 2063, 12, 9769, 306, 2139, 422, 17557, 9458, 284, 23475, 714, 3002, 5361, 3484, 290, 326, 257, 30724, 20515, 9679, 262, 2688, 8545, 8774, 6910, 379, 5882, 12735, 714, 635, 307, 40685, 13, 24811, 26, 7707, 25, 15397, 347, 1506, 16239, 11, 286, 13718, 329, 40934, 12950, 11, 531, 302, 29443, 262, 370, 8770, 1636, 6910, 561, 11169, 262, 1989, 338, 19519, 13, 10128, 347, 1506, 16239, 531, 302, 29443, 262, 1627, 561, 307, 257, 32757, 1445, 290, 1805, 262, 2095, 286, 262, 11905, 40934, 13, 46102, 364, 423, 531, 326, 302, 29443, 262, 370, 8770, 1636, 6910, 11, 543, 714, 1575, 510, 284, 4248, 3064, 76, 11, 561, 307, 257, 3236, 3034, 5750, 329, 262, 40934, 13, 32, 2050, 287, 4751, 11, 543, 373, 20462, 416, 262, 4640, 11, 11905, 40934, 4281, 11, 7215, 75, 849, 666, 4281, 290, 11905, 40934, 14973, 11, 1043, 326, 257, 2063, 12, 9769]}\n"
     ]
    }
   ],
   "source": [
    "print(lm_dataset_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124,439,808 total parameters.\n",
      "124,439,808 training parameters.\n"
     ]
    }
   ],
   "source": [
    "# Total parameters and trainable parameters.\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"{total_params:,} total parameters.\")\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"{total_trainable_params:,} training parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12620' max='12620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12620/12620 30:35, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.105200</td>\n",
       "      <td>2.861994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.946300</td>\n",
       "      <td>2.806295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.756300</td>\n",
       "      <td>2.778920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.749700</td>\n",
       "      <td>2.760299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.774400</td>\n",
       "      <td>2.749584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.658300</td>\n",
       "      <td>2.742320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.502000</td>\n",
       "      <td>2.737483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.563700</td>\n",
       "      <td>2.734116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.594100</td>\n",
       "      <td>2.729765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.605700</td>\n",
       "      <td>2.728679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.494200</td>\n",
       "      <td>2.726353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.516900</td>\n",
       "      <td>2.725620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.510700</td>\n",
       "      <td>2.724010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.515900</td>\n",
       "      <td>2.725196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.587500</td>\n",
       "      <td>2.724214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.614100</td>\n",
       "      <td>2.723371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.500200</td>\n",
       "      <td>2.725821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.456500</td>\n",
       "      <td>2.725168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.496700</td>\n",
       "      <td>2.725057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.420000</td>\n",
       "      <td>2.725326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=OUT_DIR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=OUT_DIR,\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=1,\n",
    "    report_to='tensorboard',\n",
    "    learning_rate=0.00001,\n",
    "    fp16=True,\n",
    "    dataloader_num_workers=NUM_PROCS\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_dataset_train,\n",
    "    eval_dataset=lm_dataset_valid,\n",
    ")\n",
    "\n",
    "history = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(f\"{OUT_DIR}/final_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('results_gpt2_bbc_news_summary/final_model/tokenizer_config.json',\n",
       " 'results_gpt2_bbc_news_summary/final_model/special_tokens_map.json',\n",
       " 'results_gpt2_bbc_news_summary/final_model/vocab.json',\n",
       " 'results_gpt2_bbc_news_summary/final_model/merges.txt',\n",
       " 'results_gpt2_bbc_news_summary/final_model/added_tokens.json',\n",
       " 'results_gpt2_bbc_news_summary/final_model/tokenizer.json')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(f\"{OUT_DIR}/final_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'results_gpt2_bbc_news_summary/final_model'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"American-led negotiators are edging closer to an agreement in which Israel would suspend its war in Gaza for about two months in exchange for the release of more than 100 hostages still held by Hamas, a deal that could be sealed in the next two weeks and would transform the conflict consuming the region.\n",
    "\n",
    "Negotiators have developed a written draft agreement merging proposals offered by Israel and Hamas in the last 10 days into a basic framework that will be the subject of talks in Paris on Sunday. While there are still important disagreements to be worked out, negotiators are cautiously optimistic that a final accord is within reach, according to U.S. officials who insisted on anonymity to discuss sensitive talks.\n",
    "\n",
    "President Biden spoke by phone separately Friday with the leaders of Egypt and Qatar, who have served as intermediaries with Hamas, to narrow the remaining differences. He is also sending his C.I.A. director, William J. Burns, to Paris for Sunday’s talks with Israeli, Egyptian and Qatari officials. If Mr. Burns makes enough progress, Mr. Biden may then send his Middle East coordinator, Brett McGurk, who just returned to Washington, back to the region to help finalize the agreement.\n",
    "\n",
    "“Both leaders affirmed that a hostage deal is central to establishing a prolonged humanitarian pause in the fighting and ensure additional lifesaving humanitarian assistance reaches civilians in need throughout Gaza,” the White House said in a statement Friday night summarizing the president’s conversation with Sheikh Mohammed bin Abdulrahman  al-Thani, Qatar’s prime minister. “They underscored the urgency of the situation and welcomed the close cooperation among their teams to advance recent discussions.”\n",
    "\n",
    "In a statement in Israel on Saturday, Prime Minister Benjamin Netanyahu reaffirmed his commitment to securing the release of those hostages who were not freed as part of a more limited agreement in November. “As of today, we have returned 110 of our hostages and we are committed to returning all of them home,” he said. “We are dealing with this and we are doing so around the clock, including now.”\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "American-led negotiators are edging closer to an agreement in which Israel would suspend its war in Gaza for about two months in exchange for the release of more than 100 hostages still held by Hamas, a deal that could be sealed in the next two weeks and would transform the conflict consuming the region.\n",
      "\n",
      "Negotiators have developed a written draft agreement merging proposals offered by Israel and Hamas in the last 10 days into a basic framework that will be the subject of talks in Paris on Sunday. While there are still important disagreements to be worked out, negotiators are cautiously optimistic that a final accord is within reach, according to U.S. officials who insisted on anonymity to discuss sensitive talks.\n",
      "\n",
      "President Biden spoke by phone separately Friday with the leaders of Egypt and Qatar, who have served as intermediaries with Hamas, to narrow the remaining differences. He is also sending his C.I.A. director, William J. Burns, to Paris for Sunday’s talks with Israeli, Egyptian and Qatari officials. If Mr. Burns makes enough progress, Mr. Biden may then send his Middle East coordinator, Brett McGurk, who just returned to Washington, back to the region to help finalize the agreement.\n",
      "\n",
      "“Both leaders affirmed that a hostage deal is central to establishing a prolonged humanitarian pause in the fighting and ensure additional lifesaving humanitarian assistance reaches civilians in need throughout Gaza,” the White House said in a statement Friday night summarizing the president’s conversation with Sheikh Mohammed bin Abdulrahman  al-Thani, Qatar’s prime minister. “They underscored the urgency of the situation and welcomed the close cooperation among their teams to advance recent discussions.”\n",
      "\n",
      "In a statement in Israel on Saturday, Prime Minister Benjamin Netanyahu reaffirmed his commitment to securing the release of those hostages who were not freed as part of a more limited agreement in November. “As of today, we have returned 110 of our hostages and we are committed to returning all of them home,” he said. “We are dealing with this and we are doing so around the clock, including now.”\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text, model, tokenizer, max_length=512, num_beams=5):\n",
    "    # Preprocess the text\n",
    "    inputs = tokenizer.encode(\n",
    "        text + ' TL;DR: ',\n",
    "        return_tensors='pt',\n",
    "        max_length=max_length,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Generate the summary\n",
    "        summary_ids = model.generate(\n",
    "            inputs,\n",
    "            max_length=512,\n",
    "            num_beams=num_beams,\n",
    "            early_stopping=True,\n",
    "        )\n",
    "\n",
    "    # Decode and return the summary\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'American-led negotiators are edging closer to an agreement in which Israel would suspend its war in Gaza for about two months in exchange for the release of more than 100 hostages still held by Hamas, a deal that could be sealed in the next two weeks and would transform the conflict consuming the region.\\n\\nNegotiators have developed a written draft agreement merging proposals offered by Israel and Hamas in the last 10 days into a basic framework that will be the subject of talks in Paris on Sunday. While there are still important disagreements to be worked out, negotiators are cautiously optimistic that a final accord is within reach, according to U.S. officials who insisted on anonymity to discuss sensitive talks.\\n\\nPresident Biden spoke by phone separately Friday with the leaders of Egypt and Qatar, who have served as intermediaries with Hamas, to narrow the remaining differences. He is also sending his C.I.A. director, William J. Burns, to Paris for Sunday’s talks with Israeli, Egyptian and Qatari officials. If Mr. Burns makes enough progress, Mr. Biden may then send his Middle East coordinator, Brett McGurk, who just returned to Washington, back to the region to help finalize the agreement.\\n\\n“Both leaders affirmed that a hostage deal is central to establishing a prolonged humanitarian pause in the fighting and ensure additional lifesaving humanitarian assistance reaches civilians in need throughout Gaza,” the White House said in a statement Friday night summarizing the president’s conversation with Sheikh Mohammed bin Abdulrahman  al-Thani, Qatar’s prime minister. “They underscored the urgency of the situation and welcomed the close cooperation among their teams to advance recent discussions.”\\n\\nIn a statement in Israel on Saturday, Prime Minister Benjamin Netanyahu reaffirmed his commitment to securing the release of those hostages who were not freed as part of a more limited agreement in November. “As of today, we have returned 110 of our hostages and we are committed to returning all of them home,” he said. “We are dealing with this and we are doing so around the clock, including now.” TL;DR: Both leaders reaffirmed that a hostage deal is central to establishing a prolonged humanitarian pause in the fighting and ensure additional lifesaving humanitarian assistance reaches civilians in need throughout Gaza,” the White House said in a statement Friday night summarizing the president’s conversation with Sheikh Mohammed bin Abdulrahman  al-Thani, Qatar’s prime minister.The'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize_text(prompt, model, tokenizer, num_beams=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30636,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

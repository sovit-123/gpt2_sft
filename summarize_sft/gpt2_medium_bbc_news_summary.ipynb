{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U transformers\n",
    "# !pip install -U datasets\n",
    "# !pip install tensorboard\n",
    "# !pip install sentencepiece\n",
    "# !pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    AutoModelForCausalLM\n",
    ")\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['File_path', 'Articles', 'Summaries'],\n",
      "    num_rows: 1779\n",
      "})\n",
      "Dataset({\n",
      "    features: ['File_path', 'Articles', 'Summaries'],\n",
      "    num_rows: 445\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('gopalkalpande/bbc-news-summary', split='train')\n",
    "full_dataset = dataset.train_test_split(test_size=0.2, shuffle=True)\n",
    "dataset_train = full_dataset['train']\n",
    "dataset_valid = full_dataset['test']\n",
    " \n",
    "print(dataset_train)\n",
    "print(dataset_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'File_path': 'sport', 'Articles': 'Bellamy fined after row..Newcastle have fined their Welsh striker Craig Bellamy two weeks\\' wages - about £80,000 - following his row with manager Graeme Souness...But Bellamy, 25, has not been put on the transfer list, although he did not train with the first team on Tuesday. Magpies chairman Freddy Shepherd told the Newcastle Evening Chronicle: \"It is not about money. It is about a player thinking he is bigger than this club. \"No individual is, be it the chairman, the manager or a player.\" Souness dropped Bellamy for Sunday\\'s game against Arsenal, claiming the Welshman had feigned injury after being asked to play out of position. \"When I heard what the manager was saying I was in shock,\" Bellamy said. \"I thought \\'not only has he gone behind my back, he\\'s lying\\',\" he said in response to Souness\\' remarks. And the Wales international refused to apologise. \"I won\\'t apologise because I have done nothing wrong,\" he told the Evening Chronicle. \"There\\'s no doubt about it, I am out of here.\" The difficult relationship between Souness and Bellamy boiled over at the weekend, and has led to a war of words in the media...Bellamy\\'s claim that Souness had lied about the background to the bust-up was strongly denied by Shepherd, who accused the striker of \"cheating\" the club. \"I wish to put the record straight regarding the Bellamy situation,\" said Shepherd. \"Craig walked off the training ground saying his hamstring was tight (on Friday), but what he failed to reveal was that he had told other members of the squad before training that he intended to feign injury. \"When Graeme discovered this he immediately ordered Bellamy to attend a meeting in my office. \"At that meeting Bellamy admitted to Graeme and I that he had told the players that he was going to \"fake\" an injury in training and walk off. \"He also agreed at that meeting to apologise to his team-mates for his behaviour. He didn\\'t do this which resulted in the action taken by the manager at the weekend, which I fully support. \"In my book this is cheating on the club, the supporters, the manager and his own team-mates. \"He is paid extremely well and I consider his behaviour to be totally unacceptable and totally unprofessional.\"..Bellamy\\'s latest outburst would appear to make his chances of a first-team recall remote. But even before Newcastle said the player would not be sold, Bellamy insisted he had no intention of handing in a transfer request. \"I don\\'t want the fans to think for one minute that I wouldn\\'t play for this club,\" he said. \"I\\'d play anywhere for this club, even in goal. \"It\\'s a very difficult situation for me at the moment but I\\'d never ask to leave this club. \"This club means so much to me. I couldn\\'t do it because I know I couldn\\'t come back here and play against Newcastle. It would hurt too much.\"', 'Summaries': '\"At that meeting Bellamy admitted to Graeme and I that he had told the players that he was going to \"fake\" an injury in training and walk off.\"When I heard what the manager was saying I was in shock,\" Bellamy said.But even before Newcastle said the player would not be sold, Bellamy insisted he had no intention of handing in a transfer request.Newcastle have fined their Welsh striker Craig Bellamy two weeks\\' wages - about £80,000 - following his row with manager Graeme Souness.\"In my book this is cheating on the club, the supporters, the manager and his own team-mates.Bellamy\\'s claim that Souness had lied about the background to the bust-up was strongly denied by Shepherd, who accused the striker of \"cheating\" the club.\"This club means so much to me.It is about a player thinking he is bigger than this club.\"I wish to put the record straight regarding the Bellamy situation,\" said Shepherd.\"I\\'d play anywhere for this club, even in goal.\"I don\\'t want the fans to think for one minute that I wouldn\\'t play for this club,\" he said.The difficult relationship between Souness and Bellamy boiled over at the weekend, and has led to a war of words in the media.'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'gpt2-medium'\n",
    "BATCH_SIZE = 1\n",
    "NUM_PROCS = os.cpu_count()\n",
    "EPOCHS = 3\n",
    "OUT_DIR = 'results_gpt2_medium_bbc_news_summary'\n",
    "MAX_LENGTH = 512 # Maximum context length to consider while preparing dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = dataset_train[10]['Articles'] + ' TL;DR: ' + dataset_train[10]['Summaries']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gamer buys $26,500 virtual land..A 22-year-old gamer has spent $26,500 (£13,700) on an island that exists only in a computer role-playing game (RPG)...The Australian gamer, known only by his gaming moniker Deathifier, bought the island in an online auction. The land exists within the game Project Entropia, an RPG which allows thousands of players to interact with each other. Entropia allows gamers to buy and sell virtual items using real cash, while fans of other titles often use auction site eBay to sell their virtual wares. Earlier this year economists calculated that these massively multi-player online role-playing games (MMORPGs) have a gross economic impact equivalent to the GDP of the African nation of Namibia...\"This is a historic moment in gaming history, and this sale only goes to prove that massive multi-player online gaming has reached a new plateau,\" said Marco Behrmann, director of community relations at Mindark, the game\\'s developer...The virtual island includes a gigantic abandoned castle and beautiful beaches which are described as ripe for developing beachfront property. Deathifier will make money from his investment as he is able to tax other gamers who come to his virtual land to hunt or mine for gold. He has also begun to sell plots to people who wish to build virtual homes. \"This type of investment will definitely become a trend in online gaming,\" said Deathifier. The Entopia economy lets gamers exchange real currency into PED (Project Entropia Dollars) and back again into real money. Ten PEDs are the equivalent to one US dollar and typical items sold include iron ingots ($5) and shogun armour ($1.70) Gamers can theoretically earn money by accumulating PEDs through the acquisition of goods, buildings, and land in the Entropia universe. MMORPGs have become enormously popular in the last 10 years with hundreds of thousands of gamers living out alternate lives in fantasy worlds. Almost 200,000 people are registered players on Project Entropia. TL;DR: The land exists within the game Project Entropia, an RPG which allows thousands of players to interact with each other.Entropia allows gamers to buy and sell virtual items using real cash, while fans of other titles often use auction site eBay to sell their virtual wares.Deathifier will make money from his investment as he is able to tax other gamers who come to his virtual land to hunt or mine for gold.The Australian gamer, known only by his gaming moniker Deathifier, bought the island in an online auction.\"This type of investment will definitely become a trend in online gaming,\" said Deathifier.A 22-year-old gamer has spent $26,500 (£13,700) on an island that exists only in a computer role-playing game (RPG).'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(example):\n",
    "    context = f\"{example['Articles'] + ' TL;DR: ' + example['Summaries']}\"\n",
    "    final_tokens = tokenizer(context, max_length=MAX_LENGTH, padding='max_length')\n",
    "    return final_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69923dd3dbda4f548a099777d5698389",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/1779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff7c2d1664d34e7fa5900e9a3683b36d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/445 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train = dataset_train.map(\n",
    "    preprocess_function,\n",
    "#     batched=True,\n",
    "    num_proc=NUM_PROCS,\n",
    "    remove_columns=dataset_train.column_names,\n",
    ")\n",
    "tokenized_valid = dataset_valid.map(\n",
    "    preprocess_function,\n",
    "#     batched=True,\n",
    "    num_proc=NUM_PROCS,\n",
    "    remove_columns=dataset_valid.column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [36488, 14814, 22643, 706, 5752, 492, 3791, 18676, 423, 22643, 511, 22945, 19099, 13854, 7459, 14814, 734, 2745, 6, 9400, 532, 546, 4248, 1795, 11, 830, 532, 1708, 465, 5752, 351, 4706, 7037, 34755, 311, 977, 408, 986, 1537, 7459, 14814, 11, 1679, 11, 468, 407, 587, 1234, 319, 262, 4351, 1351, 11, 3584, 339, 750, 407, 4512, 351, 262, 717, 1074, 319, 3431, 13, 2944, 79, 444, 8900, 45437, 30890, 1297, 262, 22410, 31867, 14160, 25, 366, 1026, 318, 407, 546, 1637, 13, 632, 318, 546, 257, 2137, 3612, 339, 318, 5749, 621, 428, 3430, 13, 366, 2949, 1981, 318, 11, 307, 340, 262, 8900, 11, 262, 4706, 393, 257, 2137, 526, 311, 977, 408, 5710, 7459, 14814, 329, 3502, 338, 983, 1028, 13837, 11, 8512, 262, 22945, 805, 550, 730, 3916, 5095, 706, 852, 1965, 284, 711, 503, 286, 2292, 13, 366, 2215, 314, 2982, 644, 262, 4706, 373, 2282, 314, 373, 287, 6380, 553, 7459, 14814, 531, 13, 366, 40, 1807, 705, 1662, 691, 468, 339, 3750, 2157, 616, 736, 11, 339, 338, 9105, 40264, 339, 531, 287, 2882, 284, 311, 977, 408, 6, 10252, 13, 843, 262, 11769, 3230, 6520, 284, 39012, 13, 366, 40, 1839, 470, 39012, 780, 314, 423, 1760, 2147, 2642, 553, 339, 1297, 262, 31867, 14160, 13, 366, 1858, 338, 645, 4719, 546, 340, 11, 314, 716, 503, 286, 994, 526, 383, 2408, 2776, 1022, 311, 977, 408, 290, 7459, 14814, 29939, 625, 379, 262, 5041, 11, 290, 468, 2957, 284, 257, 1175, 286, 2456, 287, 262, 2056, 986, 36488, 14814, 338, 1624, 326, 311, 977, 408, 550, 19837, 546, 262, 4469, 284, 262, 13076, 12, 929, 373, 7634, 6699, 416, 30890, 11, 508, 5371, 262, 19099, 286, 366, 2395, 803, 1, 262, 3430, 13, 366, 40, 4601, 284, 1234, 262, 1700, 3892, 5115, 262, 7459, 14814, 3074, 553, 531, 30890, 13, 366, 40441, 6807, 572, 262, 3047, 2323, 2282, 465, 36744, 373, 5381, 357, 261, 3217, 828, 475, 644, 339, 4054, 284, 7766, 373, 326, 339, 550, 1297, 584, 1866, 286, 262, 8244, 878, 3047, 326, 339, 5292, 284, 730, 570, 5095, 13, 366, 2215, 7037, 34755, 5071, 428, 339, 3393, 6149, 7459, 14814, 284, 5262, 257, 3249, 287, 616, 2607, 13, 366, 2953, 326, 3249, 7459, 14814, 6848, 284, 7037, 34755, 290, 314, 326, 339, 550, 1297, 262, 1938, 326, 339, 373, 1016, 284, 366, 30706, 1, 281, 5095, 287, 3047, 290, 2513, 572, 13, 366, 1544, 635, 4987, 379, 326, 3249, 284, 39012, 284, 465, 1074, 12, 7300, 329, 465, 9172, 13, 679, 1422, 470, 466, 428, 543, 8724, 287, 262, 2223, 2077, 416, 262, 4706, 379, 262, 5041, 11, 543, 314, 3938, 1104, 13, 366, 818, 616, 1492, 428, 318, 21608, 319, 262, 3430, 11, 262, 5941, 11, 262, 4706, 290, 465, 898, 1074, 12, 7300, 13, 366, 1544, 318, 3432, 4457, 880, 290, 314, 2074, 465, 9172, 284, 307, 6635, 18010, 290, 6635, 555, 33163, 526, 492, 36488, 14814, 338, 3452, 49088, 561, 1656, 284, 787, 465, 8395, 286, 257, 717, 12, 15097, 10014, 6569, 13, 887, 772, 878, 22410, 531, 262, 2137, 561, 407, 307, 2702, 11, 7459, 14814, 11189, 339, 550, 645, 6778, 286, 22786, 287, 257, 4351, 2581, 13, 366, 40, 836, 470, 765, 262, 3296, 284, 892, 329, 530, 5664, 326, 314, 3636, 470, 711, 329, 428, 3430, 553, 339, 531, 13, 366, 40, 1549, 711, 6609, 329, 428, 3430, 11, 772, 287, 3061, 13, 366, 1026, 338, 257, 845, 2408, 3074, 329, 502, 379, 262, 2589, 475, 314, 1549, 1239, 1265, 284, 2666, 428, 3430, 13, 366, 1212, 3430, 1724, 523, 881, 284, 502, 13, 314, 3521, 470, 466, 340, 780, 314, 760, 314, 3521, 470, 1282, 736, 994, 290, 711, 1028, 22410, 13, 632, 561, 5938, 1165, 881, 526, 24811, 26, 7707, 25, 366, 2953, 326, 3249, 7459, 14814, 6848, 284, 7037, 34755, 290, 314, 326, 339, 550, 1297, 262, 1938, 326, 339, 373, 1016, 284, 366, 30706, 1, 281, 5095, 287, 3047, 290, 2513, 572, 526, 2215, 314, 2982, 644, 262, 4706, 373, 2282, 314, 373, 287, 6380, 553, 7459, 14814, 531, 13, 1537, 772, 878, 22410, 531, 262, 2137, 561, 407, 307, 2702, 11, 7459, 14814, 11189, 339, 550, 645, 6778, 286, 22786, 287, 257, 4351, 2581, 13, 3791, 18676, 423, 22643, 511, 22945, 19099, 13854, 7459, 14814, 734, 2745, 6, 9400, 532, 546, 4248, 1795, 11, 830, 532, 1708, 465, 5752, 351, 4706, 7037, 34755, 311, 977, 408, 526, 818, 616, 1492, 428, 318, 21608, 319, 262, 3430, 11, 262, 5941, 11, 262, 4706, 290, 465, 898, 1074, 12, 7300, 13, 36488, 14814, 338, 1624, 326, 311, 977, 408, 550, 19837, 546, 262, 4469, 284, 262, 13076, 12, 929, 373, 7634, 6699, 416, 30890, 11, 508, 5371, 262, 19099, 286, 366, 2395, 803, 1, 262, 3430, 526, 1212, 3430, 1724, 523, 881, 284, 502, 13, 1026, 318, 546, 257, 2137, 3612, 339, 318, 5749, 621, 428, 3430, 526, 40, 4601, 284, 1234, 262, 1700, 3892, 5115, 262, 7459, 14814, 3074, 553, 531, 30890, 526, 40, 1549, 711, 6609, 329, 428, 3430, 11, 772, 287, 3061, 526, 40, 836, 470, 765, 262, 3296, 284, 892, 329, 530, 5664, 326, 314, 3636, 470, 711, 329, 428, 3430, 553, 339, 531, 13, 464, 2408, 2776, 1022, 311, 977, 408, 290, 7459, 14814, 29939, 625, 379, 262, 5041, 11, 290, 468, 2957, 284, 257, 1175, 286, 2456, 287, 262, 2056, 13], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    if total_length >= MAX_LENGTH:\n",
    "        total_length = (total_length // MAX_LENGTH) * MAX_LENGTH\n",
    "    result = {\n",
    "        k: [t[i : i + MAX_LENGTH] for i in range(0, total_length, MAX_LENGTH)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbbabf73df844159a37804bcb9642c50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/1779 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "198aa321583d401a923720d481b48e24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/445 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm_dataset_train = tokenized_train.map(\n",
    "    group_texts, num_proc=NUM_PROCS, batched=True\n",
    ")\n",
    "lm_dataset_valid = tokenized_valid.map(\n",
    "    group_texts, num_proc=NUM_PROCS, batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [36488, 14814, 22643, 706, 5752, 492, 3791, 18676, 423, 22643, 511, 22945, 19099, 13854, 7459, 14814, 734, 2745, 6, 9400, 532, 546, 4248, 1795, 11, 830, 532, 1708, 465, 5752, 351, 4706, 7037, 34755, 311, 977, 408, 986, 1537, 7459, 14814, 11, 1679, 11, 468, 407, 587, 1234, 319, 262, 4351, 1351, 11, 3584, 339, 750, 407, 4512, 351, 262, 717, 1074, 319, 3431, 13, 2944, 79, 444, 8900, 45437, 30890, 1297, 262, 22410, 31867, 14160, 25, 366, 1026, 318, 407, 546, 1637, 13, 632, 318, 546, 257, 2137, 3612, 339, 318, 5749, 621, 428, 3430, 13, 366, 2949, 1981, 318, 11, 307, 340, 262, 8900, 11, 262, 4706, 393, 257, 2137, 526, 311, 977, 408, 5710, 7459, 14814, 329, 3502, 338, 983, 1028, 13837, 11, 8512, 262, 22945, 805, 550, 730, 3916, 5095, 706, 852, 1965, 284, 711, 503, 286, 2292, 13, 366, 2215, 314, 2982, 644, 262, 4706, 373, 2282, 314, 373, 287, 6380, 553, 7459, 14814, 531, 13, 366, 40, 1807, 705, 1662, 691, 468, 339, 3750, 2157, 616, 736, 11, 339, 338, 9105, 40264, 339, 531, 287, 2882, 284, 311, 977, 408, 6, 10252, 13, 843, 262, 11769, 3230, 6520, 284, 39012, 13, 366, 40, 1839, 470, 39012, 780, 314, 423, 1760, 2147, 2642, 553, 339, 1297, 262, 31867, 14160, 13, 366, 1858, 338, 645, 4719, 546, 340, 11, 314, 716, 503, 286, 994, 526, 383, 2408, 2776, 1022, 311, 977, 408, 290, 7459, 14814, 29939, 625, 379, 262, 5041, 11, 290, 468, 2957, 284, 257, 1175, 286, 2456, 287, 262, 2056, 986, 36488, 14814, 338, 1624, 326, 311, 977, 408, 550, 19837, 546, 262, 4469, 284, 262, 13076, 12, 929, 373, 7634, 6699, 416, 30890, 11, 508, 5371, 262, 19099, 286, 366, 2395, 803, 1, 262, 3430, 13, 366, 40, 4601, 284, 1234, 262, 1700, 3892, 5115, 262, 7459, 14814, 3074, 553, 531, 30890, 13, 366, 40441, 6807, 572, 262, 3047, 2323, 2282, 465, 36744, 373, 5381, 357, 261, 3217, 828, 475, 644, 339, 4054, 284, 7766, 373, 326, 339, 550, 1297, 584, 1866, 286, 262, 8244, 878, 3047, 326, 339, 5292, 284, 730, 570, 5095, 13, 366, 2215, 7037, 34755, 5071, 428, 339, 3393, 6149, 7459, 14814, 284, 5262, 257, 3249, 287, 616, 2607, 13, 366, 2953, 326, 3249, 7459, 14814, 6848, 284, 7037, 34755, 290, 314, 326, 339, 550, 1297, 262, 1938, 326, 339, 373, 1016, 284, 366, 30706, 1, 281, 5095, 287, 3047, 290, 2513, 572, 13, 366, 1544, 635, 4987, 379, 326, 3249, 284, 39012, 284, 465, 1074, 12, 7300, 329, 465, 9172, 13, 679, 1422, 470, 466, 428, 543, 8724, 287, 262, 2223, 2077, 416, 262, 4706, 379, 262, 5041, 11, 543, 314, 3938, 1104, 13, 366, 818, 616, 1492, 428, 318, 21608, 319, 262, 3430, 11, 262, 5941, 11, 262, 4706, 290, 465, 898, 1074, 12, 7300, 13, 366, 1544, 318, 3432, 4457, 880, 290, 314, 2074, 465, 9172, 284, 307, 6635, 18010, 290, 6635, 555, 33163, 526, 492, 36488, 14814, 338, 3452, 49088, 561, 1656, 284, 787, 465, 8395, 286, 257, 717, 12, 15097, 10014, 6569, 13, 887, 772, 878], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [36488, 14814, 22643, 706, 5752, 492, 3791, 18676, 423, 22643, 511, 22945, 19099, 13854, 7459, 14814, 734, 2745, 6, 9400, 532, 546, 4248, 1795, 11, 830, 532, 1708, 465, 5752, 351, 4706, 7037, 34755, 311, 977, 408, 986, 1537, 7459, 14814, 11, 1679, 11, 468, 407, 587, 1234, 319, 262, 4351, 1351, 11, 3584, 339, 750, 407, 4512, 351, 262, 717, 1074, 319, 3431, 13, 2944, 79, 444, 8900, 45437, 30890, 1297, 262, 22410, 31867, 14160, 25, 366, 1026, 318, 407, 546, 1637, 13, 632, 318, 546, 257, 2137, 3612, 339, 318, 5749, 621, 428, 3430, 13, 366, 2949, 1981, 318, 11, 307, 340, 262, 8900, 11, 262, 4706, 393, 257, 2137, 526, 311, 977, 408, 5710, 7459, 14814, 329, 3502, 338, 983, 1028, 13837, 11, 8512, 262, 22945, 805, 550, 730, 3916, 5095, 706, 852, 1965, 284, 711, 503, 286, 2292, 13, 366, 2215, 314, 2982, 644, 262, 4706, 373, 2282, 314, 373, 287, 6380, 553, 7459, 14814, 531, 13, 366, 40, 1807, 705, 1662, 691, 468, 339, 3750, 2157, 616, 736, 11, 339, 338, 9105, 40264, 339, 531, 287, 2882, 284, 311, 977, 408, 6, 10252, 13, 843, 262, 11769, 3230, 6520, 284, 39012, 13, 366, 40, 1839, 470, 39012, 780, 314, 423, 1760, 2147, 2642, 553, 339, 1297, 262, 31867, 14160, 13, 366, 1858, 338, 645, 4719, 546, 340, 11, 314, 716, 503, 286, 994, 526, 383, 2408, 2776, 1022, 311, 977, 408, 290, 7459, 14814, 29939, 625, 379, 262, 5041, 11, 290, 468, 2957, 284, 257, 1175, 286, 2456, 287, 262, 2056, 986, 36488, 14814, 338, 1624, 326, 311, 977, 408, 550, 19837, 546, 262, 4469, 284, 262, 13076, 12, 929, 373, 7634, 6699, 416, 30890, 11, 508, 5371, 262, 19099, 286, 366, 2395, 803, 1, 262, 3430, 13, 366, 40, 4601, 284, 1234, 262, 1700, 3892, 5115, 262, 7459, 14814, 3074, 553, 531, 30890, 13, 366, 40441, 6807, 572, 262, 3047, 2323, 2282, 465, 36744, 373, 5381, 357, 261, 3217, 828, 475, 644, 339, 4054, 284, 7766, 373, 326, 339, 550, 1297, 584, 1866, 286, 262, 8244, 878, 3047, 326, 339, 5292, 284, 730, 570, 5095, 13, 366, 2215, 7037, 34755, 5071, 428, 339, 3393, 6149, 7459, 14814, 284, 5262, 257, 3249, 287, 616, 2607, 13, 366, 2953, 326, 3249, 7459, 14814, 6848, 284, 7037, 34755, 290, 314, 326, 339, 550, 1297, 262, 1938, 326, 339, 373, 1016, 284, 366, 30706, 1, 281, 5095, 287, 3047, 290, 2513, 572, 13, 366, 1544, 635, 4987, 379, 326, 3249, 284, 39012, 284, 465, 1074, 12, 7300, 329, 465, 9172, 13, 679, 1422, 470, 466, 428, 543, 8724, 287, 262, 2223, 2077, 416, 262, 4706, 379, 262, 5041, 11, 543, 314, 3938, 1104, 13, 366, 818, 616, 1492, 428, 318, 21608, 319, 262, 3430, 11, 262, 5941, 11, 262, 4706, 290, 465, 898, 1074, 12, 7300, 13, 366, 1544, 318, 3432, 4457, 880, 290, 314, 2074, 465, 9172, 284, 307, 6635, 18010, 290, 6635, 555, 33163, 526, 492, 36488, 14814, 338, 3452, 49088, 561, 1656, 284, 787, 465, 8395, 286, 257, 717, 12, 15097, 10014, 6569, 13, 887, 772, 878]}\n"
     ]
    }
   ],
   "source": [
    "print(lm_dataset_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "354,823,168 total parameters.\n",
      "354,823,168 training parameters.\n"
     ]
    }
   ],
   "source": [
    "# Total parameters and trainable parameters.\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"{total_params:,} total parameters.\")\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"{total_trainable_params:,} training parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7638' max='7638' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7638/7638 19:22, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.784800</td>\n",
       "      <td>2.503491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.670900</td>\n",
       "      <td>2.469852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.407400</td>\n",
       "      <td>2.466026</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=OUT_DIR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=OUT_DIR,\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=1,\n",
    "    report_to='tensorboard',\n",
    "    learning_rate=0.00001,\n",
    "    fp16=True,\n",
    "    dataloader_num_workers=NUM_PROCS\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_dataset_train,\n",
    "    eval_dataset=lm_dataset_valid,\n",
    ")\n",
    "\n",
    "history = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(f\"{OUT_DIR}/final_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('results_gpt2_bbc_news_summary/final_model/tokenizer_config.json',\n",
       " 'results_gpt2_bbc_news_summary/final_model/special_tokens_map.json',\n",
       " 'results_gpt2_bbc_news_summary/final_model/vocab.json',\n",
       " 'results_gpt2_bbc_news_summary/final_model/merges.txt',\n",
       " 'results_gpt2_bbc_news_summary/final_model/added_tokens.json',\n",
       " 'results_gpt2_bbc_news_summary/final_model/tokenizer.json')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(f\"{OUT_DIR}/final_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'results_gpt2_medium_bbc_news_summary/final_model'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 1024)\n",
       "    (wpe): Embedding(1024, 1024)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"American-led negotiators are edging closer to an agreement in which Israel would suspend its war in Gaza for about two months in exchange for the release of more than 100 hostages still held by Hamas, a deal that could be sealed in the next two weeks and would transform the conflict consuming the region.\n",
    "\n",
    "Negotiators have developed a written draft agreement merging proposals offered by Israel and Hamas in the last 10 days into a basic framework that will be the subject of talks in Paris on Sunday. While there are still important disagreements to be worked out, negotiators are cautiously optimistic that a final accord is within reach, according to U.S. officials who insisted on anonymity to discuss sensitive talks.\n",
    "\n",
    "President Biden spoke by phone separately Friday with the leaders of Egypt and Qatar, who have served as intermediaries with Hamas, to narrow the remaining differences. He is also sending his C.I.A. director, William J. Burns, to Paris for Sunday’s talks with Israeli, Egyptian and Qatari officials. If Mr. Burns makes enough progress, Mr. Biden may then send his Middle East coordinator, Brett McGurk, who just returned to Washington, back to the region to help finalize the agreement.\n",
    "\n",
    "“Both leaders affirmed that a hostage deal is central to establishing a prolonged humanitarian pause in the fighting and ensure additional lifesaving humanitarian assistance reaches civilians in need throughout Gaza,” the White House said in a statement Friday night summarizing the president’s conversation with Sheikh Mohammed bin Abdulrahman  al-Thani, Qatar’s prime minister. “They underscored the urgency of the situation and welcomed the close cooperation among their teams to advance recent discussions.”\n",
    "\n",
    "In a statement in Israel on Saturday, Prime Minister Benjamin Netanyahu reaffirmed his commitment to securing the release of those hostages who were not freed as part of a more limited agreement in November. “As of today, we have returned 110 of our hostages and we are committed to returning all of them home,” he said. “We are dealing with this and we are doing so around the clock, including now.”\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "American-led negotiators are edging closer to an agreement in which Israel would suspend its war in Gaza for about two months in exchange for the release of more than 100 hostages still held by Hamas, a deal that could be sealed in the next two weeks and would transform the conflict consuming the region.\n",
      "\n",
      "Negotiators have developed a written draft agreement merging proposals offered by Israel and Hamas in the last 10 days into a basic framework that will be the subject of talks in Paris on Sunday. While there are still important disagreements to be worked out, negotiators are cautiously optimistic that a final accord is within reach, according to U.S. officials who insisted on anonymity to discuss sensitive talks.\n",
      "\n",
      "President Biden spoke by phone separately Friday with the leaders of Egypt and Qatar, who have served as intermediaries with Hamas, to narrow the remaining differences. He is also sending his C.I.A. director, William J. Burns, to Paris for Sunday’s talks with Israeli, Egyptian and Qatari officials. If Mr. Burns makes enough progress, Mr. Biden may then send his Middle East coordinator, Brett McGurk, who just returned to Washington, back to the region to help finalize the agreement.\n",
      "\n",
      "“Both leaders affirmed that a hostage deal is central to establishing a prolonged humanitarian pause in the fighting and ensure additional lifesaving humanitarian assistance reaches civilians in need throughout Gaza,” the White House said in a statement Friday night summarizing the president’s conversation with Sheikh Mohammed bin Abdulrahman  al-Thani, Qatar’s prime minister. “They underscored the urgency of the situation and welcomed the close cooperation among their teams to advance recent discussions.”\n",
      "\n",
      "In a statement in Israel on Saturday, Prime Minister Benjamin Netanyahu reaffirmed his commitment to securing the release of those hostages who were not freed as part of a more limited agreement in November. “As of today, we have returned 110 of our hostages and we are committed to returning all of them home,” he said. “We are dealing with this and we are doing so around the clock, including now.”\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text, model, tokenizer, max_length=512, num_beams=5):\n",
    "    # Preprocess the text\n",
    "    inputs = tokenizer.encode(\n",
    "        text + ' TL;DR: ',\n",
    "        return_tensors='pt',\n",
    "        max_length=max_length,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Generate the summary\n",
    "        summary_ids = model.generate(\n",
    "            inputs,\n",
    "            max_length=512,\n",
    "            num_beams=num_beams,\n",
    "            early_stopping=True,\n",
    "        )\n",
    "\n",
    "    # Decode and return the summary\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"American-led negotiators are edging closer to an agreement in which Israel would suspend its war in Gaza for about two months in exchange for the release of more than 100 hostages still held by Hamas, a deal that could be sealed in the next two weeks and would transform the conflict consuming the region.\\n\\nNegotiators have developed a written draft agreement merging proposals offered by Israel and Hamas in the last 10 days into a basic framework that will be the subject of talks in Paris on Sunday. While there are still important disagreements to be worked out, negotiators are cautiously optimistic that a final accord is within reach, according to U.S. officials who insisted on anonymity to discuss sensitive talks.\\n\\nPresident Biden spoke by phone separately Friday with the leaders of Egypt and Qatar, who have served as intermediaries with Hamas, to narrow the remaining differences. He is also sending his C.I.A. director, William J. Burns, to Paris for Sunday’s talks with Israeli, Egyptian and Qatari officials. If Mr. Burns makes enough progress, Mr. Biden may then send his Middle East coordinator, Brett McGurk, who just returned to Washington, back to the region to help finalize the agreement.\\n\\n“Both leaders affirmed that a hostage deal is central to establishing a prolonged humanitarian pause in the fighting and ensure additional lifesaving humanitarian assistance reaches civilians in need throughout Gaza,” the White House said in a statement Friday night summarizing the president’s conversation with Sheikh Mohammed bin Abdulrahman  al-Thani, Qatar’s prime minister. “They underscored the urgency of the situation and welcomed the close cooperation among their teams to advance recent discussions.”\\n\\nIn a statement in Israel on Saturday, Prime Minister Benjamin Netanyahu reaffirmed his commitment to securing the release of those hostages who were not freed as part of a more limited agreement in November. “As of today, we have returned 110 of our hostages and we are committed to returning all of them home,” he said. “We are dealing with this and we are doing so around the clock, including now.” TL;DR: Both leaders affirmed that a hostage deal is central to establishing a prolonged humanitarian pause in the fighting and ensure additional lifesaving humanitarian assistance reaches civilians in need throughout Gaza,” the White House said in a statement Friday night summarizing the president’s conversation with Sheikh Mohammed bin Abdulrahman al-Thani, Qatar's prime minister.In a statement in Israel\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize_text(prompt, model, tokenizer, num_beams=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30636,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

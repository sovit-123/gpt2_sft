{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "724c95c3-b4c3-437e-b2e1-32c757dd140d",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "This notebook replicates a simple chat template with continuous chat. The model understands and remembers the context according to its capacity.\n",
    "\n",
    "This is a OPT 125M model fine-tuned to Chat Alpaca dataset (https://huggingface.co/datasets/flpelerin/ChatAlpaca-10k). Find the fine-tuning notebook in the `assistant_sft` directory.\n",
    "\n",
    "**NOTE: The notebook uses a customized streamer for text streaming.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a77e0de-b8eb-413a-b244-6e69fb978590",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "\n",
    "from streaming_utils import TextStreamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "034fbac3-e2dc-4679-b4e5-6f66c04ff203",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    '../examples/assistant_sft/opt_125m_chat_alpaca/outputs/opt_125m_chat_alpaca/best_model/',\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    '../examples/assistant_sft/opt_125m_chat_alpaca/outputs/opt_125m_chat_alpaca/best_model/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "259ed1c4-f923-44da-852b-efa82f01ad29",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer = TextStreamer(\n",
    "    tokenizer, \n",
    "    skip_prompt=True, \n",
    "    skip_special_tokens=True, \n",
    "    truncate_before_pattern=['\\[\\/'],\n",
    "    truncate=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "238f0eb5-b75a-4c71-985d-999af3ab2080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.eos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8be0f4be-dba3-4cbc-b354-c27213a87d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.set_verbosity(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c5afc34-37c3-4766-84d7-651207b6e0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# template = \"\"\"</s>[INST] {prompt} [/INST]\"\"\"\n",
    "eos_string = tokenizer.eos_token\n",
    "history = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6765c16-5510-4057-8dce-e1db702ad02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90611237-81ca-4b55-9cec-27b1a2e759c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Question:  Hello\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I am writing to inform you that the following email is a spam email: I am writing to inform you that I am not able to send you any emails. I am an AI language model and do not have any specific information about you. Please check your email for any spam emails.\n",
      "##################################################\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Question:  Let's talk about deep learning/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Deep learning is a type of machine learning that uses neural networks to learn from data. It is a powerful tool that can be used to improve the accuracy of machine learning models. Deep learning models can learn from large amounts of data, such as text, images, and other types of data, and then use this data to make predictions or decisions. Deep learning models can also be trained on large datasets, such as large datasets of customer data, to improve their accuracy.\n",
      "##################################################\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Question:  What is 2+2?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2+2 is a term that refers to a set of data that is not directly related to the model. It is often used to describe a set of data that is not directly related to the model. For example, a dataset of customer reviews or customer service interactions can be used to train deep learning models. \n",
      "\n",
      "2+2 can also be used to describe a set of data that is not directly related to the model. For example, a dataset of customer reviews or customer service interactions can be used to train deep learning models. \n",
      "\n",
      "In summary, 2+2 is a commonly used term in machine learning. It is often used to describe a set of data that is not directly related to the model.\n",
      "##################################################\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 2\u001b[0m     question\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mQuestion: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m history \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/lm_sft/lib/python3.11/site-packages/ipykernel/kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/lm_sft/lib/python3.11/site-packages/ipykernel/kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    question=input(\"Question: \")\n",
    "    inputs = ''\n",
    "\n",
    "    if history is None:\n",
    "        template = \"\"\"</s>[INST] {prompt} [/INST]\"\"\"\n",
    "    else:\n",
    "        template = \"\"\"[INST] {prompt} [/INST]\"\"\"\n",
    "\n",
    "    prompt = history + ' ' + template.format(prompt=question, inputs=inputs) if history is not None else template.format(prompt=question, inputs=inputs)\n",
    "\n",
    "    # print(f\"PROMPT: {prompt}\")\n",
    "\n",
    "    prompt_tokenized = tokenizer(prompt, return_tensors='pt')['input_ids']\n",
    "    \n",
    "    output_tokenized = model.generate(\n",
    "        input_ids=prompt_tokenized, \n",
    "        max_length=len(prompt_tokenized[0])+256,\n",
    "        temperature=0.7,\n",
    "        top_k=40,\n",
    "        top_p=0.1,\n",
    "        do_sample=True,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        streamer=streamer\n",
    "    )\n",
    "    answer = tokenizer.decode(token_ids=output_tokenized[0][len(prompt_tokenized[0]):]).strip()\n",
    "    \n",
    "    if eos_string in answer:\n",
    "        answer = answer.split(eos_string)[0].strip()\n",
    "    if '[/' in answer:\n",
    "        answer = answer.split('[/')[0].strip()\n",
    "\n",
    "    history = ' '.join([prompt, answer, eos_string])\n",
    "    # print(f\"ANSWER: {answer}\\n\")\n",
    "    # print(f\"HISTORY: {history}\\n\")\n",
    "    print('#' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbbcaa4-9bfd-4265-8fd0-5bd5a18808bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f2cb63-121a-4b8f-a12d-b1d3a429e94e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
